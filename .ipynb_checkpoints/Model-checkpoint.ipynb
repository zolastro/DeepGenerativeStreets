{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z6amseO5JQaf"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import torchvision.models as models\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from  torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from skimage import io, transform\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import math\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DgQqlTPjJQam"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (<ipython-input-88-23d1d279ff85>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-88-23d1d279ff85>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    def __init__(self, outer_nf, inner_nf, input_nf=None, outermost=False, innermost=False,\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "class UnetSkipConnectionBlock(nn.Module):\n",
    "    def __init__(self, outer_nf, inner_nf, input_nf=None, outermost=False, innermost=False,\n",
    "                 submodule=None, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "        super(UnetSkipConnectionBlock, self).__init__()\n",
    "        self.outermost = outermost\n",
    "        if input_nf is None:\n",
    "            input_nf = outer_nf\n",
    "        downconv = nn.Conv2d(input_nf, inner_nf, kernel_size=4, \n",
    "                             stride=2, padding=1, bias=False)\n",
    "        downrelu = nn.LeakyReLU(0.2, True)\n",
    "        downnorm= norm_layer(inner_nf)\n",
    "        uprelu = nn.ReLU(True)\n",
    "        upnorm = norm_layer(outer_nf)\n",
    "\n",
    "        if outermost:\n",
    "            upconv = nn.ConvTranspose2d(inner_nf * 2, outer_nf,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1)\n",
    "            down = [downconv]\n",
    "            up = [uprelu, upconv, nn.Tanh()]\n",
    "            model = down + [submodule] + up\n",
    "        elif innermost:\n",
    "            upconv = nn.ConvTranspose2d(inner_nf, outer_nf,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1, bias=False)\n",
    "            down = [downrelu, downconv]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            model = down + up\n",
    "        else:\n",
    "            upconv = nn.ConvTranspose2d(inner_nf * 2, outer_nf,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1, bias=False)\n",
    "            down = [downrelu, downconv, downnorm]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "\n",
    "            if use_dropout:\n",
    "                model = down + [submodule] + up + [nn.Dropout(0.5)]\n",
    "            else:\n",
    "                model = down + [submodule] + up\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.outermost:\n",
    "            return self.model(x)\n",
    "        else:\n",
    "            return torch.cat([x, self.model(x)], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hFJz4rIZJQau"
   },
   "outputs": [],
   "source": [
    "class UnetGenerator(nn.Module):\n",
    "    def __init__(self, c_input, c_output, n_downsampling, n_filters,\n",
    "                norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "        super(UnetGenerator, self).__init__()\n",
    "        unet_block = UnetSkipConnectionBlock(n_filters*8, n_filters*8, input_nf=None, submodule=None, norm_layer = norm_layer, innermost=True)\n",
    "        for i in range(n_downsampling - 5):\n",
    "            unet_block = UnetSkipConnectionBlock(n_filters*8, n_filters*8, input_nf=None, submodule=unet_block, norm_layer = norm_layer, use_dropout)\n",
    "        unet_block = UnetSkipConnectionBlock(n_filters*4, n_filters*8, input_nf=None, submodule=unet_block, norm_layer = norm_layer, use_dropout)\n",
    "        unet_block = UnetSkipConnectionBlock(n_filters*2, n_filters*4, input_nf=None, submodule=unet_block, norm_layer = norm_layer, use_dropout)\n",
    "        unet_block = UnetSkipConnectionBlock(n_filters, n_filters*2, input_nf=None, submodule=unet_block, norm_layer = norm_layer, use_dropout)\n",
    "        self.model = UnetSkipConnectionBlock(c_output, n_filters, input_nf=c_input, submodule=unet_block, norm_layer = norm_layer, outermost=True, use_dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gMvT-htcJQaz"
   },
   "outputs": [],
   "source": [
    "class NLayerDiscriminator(nn.Module):\n",
    "    def __init__(self, c_input, n_filters, n_layers, norm_layer=nn.BatchNorm2d):\n",
    "        super(NLayerDiscriminator, self).__init__()\n",
    "        use_bias=False\n",
    "        kw = 4\n",
    "        padw = 1\n",
    "        sequence = [nn.Conv2d(c_input, n_filters, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n",
    "        nf_mult = 1\n",
    "        nf_mult_prev = 1\n",
    "        for n in range(1, n_layers):  # gradually increase the number of filters\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2 ** n, 8)\n",
    "            sequence += [\n",
    "                nn.Conv2d(n_filters * nf_mult_prev, n_filters * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
    "                norm_layer(n_filters * nf_mult),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ]\n",
    "\n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2 ** n_layers, 8)\n",
    "        sequence += [\n",
    "            nn.Conv2d(n_filters * nf_mult_prev, n_filters * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
    "            norm_layer(n_filters * nf_mult),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        sequence += [nn.Conv2d(n_filters * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  # output 1 channel prediction map\n",
    "        self.model = nn.Sequential(*sequence)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"Standard forward.\"\"\"\n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2_n1N9AkJQa9"
   },
   "outputs": [],
   "source": [
    "class GANLoss(nn.Module):\n",
    "    \"\"\"Define different GAN objectives.\n",
    "    The GANLoss class abstracts away the need to create the target label tensor\n",
    "    that has the same size as the input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gan_mode, target_real_label=1.0, target_fake_label=0.0):\n",
    "        \"\"\" Initialize the GANLoss class.\n",
    "        Parameters:\n",
    "            gan_mode (str) - - the type of GAN objective. It currently supports vanilla, lsgan, and wgangp.\n",
    "            target_real_label (bool) - - label for a real image\n",
    "            target_fake_label (bool) - - label of a fake image\n",
    "        Note: Do not use sigmoid as the last layer of Discriminator.\n",
    "        LSGAN needs no sigmoid. vanilla GANs will handle it with BCEWithLogitsLoss.\n",
    "        \"\"\"\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.register_buffer('real_label', torch.tensor(target_real_label))\n",
    "        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n",
    "        self.gan_mode = gan_mode\n",
    "        if gan_mode == 'lsgan':\n",
    "            self.loss = nn.MSELoss()\n",
    "        elif gan_mode == 'vanilla':\n",
    "            self.loss = nn.BCEWithLogitsLoss()\n",
    "        elif gan_mode in ['wgangp']:\n",
    "            self.loss = None\n",
    "        else:\n",
    "            raise NotImplementedError('gan mode %s not implemented' % gan_mode)\n",
    "\n",
    "    def get_target_tensor(self, prediction, target_is_real):\n",
    "        \"\"\"Create label tensors with the same size as the input.\n",
    "        Parameters:\n",
    "            prediction (tensor) - - tpyically the prediction from a discriminator\n",
    "            target_is_real (bool) - - if the ground truth label is for real images or fake images\n",
    "        Returns:\n",
    "            A label tensor filled with ground truth label, and with the size of the input\n",
    "        \"\"\"\n",
    "\n",
    "        if target_is_real:\n",
    "            target_tensor = self.real_label\n",
    "        else:\n",
    "            target_tensor = self.fake_label\n",
    "        return target_tensor.expand_as(prediction)\n",
    "\n",
    "    def __call__(self, prediction, target_is_real):\n",
    "        \"\"\"Calculate loss given Discriminator's output and grount truth labels.\n",
    "        Parameters:\n",
    "            prediction (tensor) - - tpyically the prediction output from a discriminator\n",
    "            target_is_real (bool) - - if the ground truth label is for real images or fake images\n",
    "        Returns:\n",
    "            the calculated loss.\n",
    "        \"\"\"\n",
    "        if self.gan_mode in ['lsgan', 'vanilla']:\n",
    "            target_tensor = self.get_target_tensor(prediction, target_is_real)\n",
    "            loss = self.loss(prediction, target_tensor)\n",
    "        elif self.gan_mode == 'wgangp':\n",
    "            if target_is_real:\n",
    "                loss = -prediction.mean()\n",
    "            else:\n",
    "                loss = prediction.mean()\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aDCF18bVJQbG"
   },
   "outputs": [],
   "source": [
    "    class Pix2Pix(nn.Module):\n",
    "        def __init__(self, opt):\n",
    "            super(Pix2Pix, self).__init__()\n",
    "            self.opt = opt\n",
    "            self.gpu_ids = opt['gpu_ids']\n",
    "            self.isTrain = opt['isTrain']\n",
    "            self.device = torch.device('cuda:{}'.format(self.gpu_ids[0])) if self.gpu_ids else torch.device('cpu')  # get device name: CPU or GPU\n",
    "\n",
    "            # Generator network\n",
    "            self.use_dropout = opt['use_dropout']\n",
    "            self.netG = UnetGenerator(opt['c_input'], opt['c_output'], opt['n_downsampling'], opt['ng_filters'], nn.BatchNorm2d, self.use_dropout)\n",
    "\n",
    "            # Discriminator network\n",
    "            self.netD = NLayerDiscriminator(opt['c_input'] + opt['c_output'], opt['nd_filters'], opt['nd_layers'], nn.BatchNorm2d)\n",
    "            self.lossGAN = GANLoss('vanilla').to(self.device)\n",
    "            self.lossL1 = torch.nn.L1Loss()\n",
    "\n",
    "            self.optimizer_G = torch.optim.Adam(self.netG.parameters(), lr=opt['lr'], betas=(opt['beta1'], 0.999))\n",
    "            self.optimizer_D = torch.optim.Adam(self.netD.parameters(), lr=opt['lr'], betas=(opt['beta1'], 0.999))\n",
    "\n",
    "            if len(self.gpu_ids) > 0:\n",
    "                assert(torch.cuda.is_available())\n",
    "                self.netG.to(self.gpu_ids[0])\n",
    "                self.netG = torch.nn.DataParallel(self.netG, self.gpu_ids)\n",
    "                self.netD.to(self.gpu_ids[0])\n",
    "                self.netD = torch.nn.DataParallel(self.netD, self.gpu_ids)\n",
    "            \n",
    "\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"Run forward pass; called by both functions <optimize_parameters> and <test>.\"\"\"\n",
    "            x = x.to(self.device)\n",
    "            return self.netG(x)  # G(A)\n",
    "\n",
    "        def set_requires_grad(self, nets, requires_grad=False):\n",
    "            \"\"\"Set requies_grad=Fasle for all the networks to avoid unnecessary computations\n",
    "            Parameters:\n",
    "                nets (network list)   -- a list of networks\n",
    "                requires_grad (bool)  -- whether the networks require gradients or not\n",
    "            \"\"\"\n",
    "            if not isinstance(nets, list):\n",
    "                nets = [nets]\n",
    "            for net in nets:\n",
    "                if net is not None:\n",
    "                    for param in net.parameters():\n",
    "                        param.requires_grad = requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iF8wPeu5JQbL"
   },
   "outputs": [],
   "source": [
    "opt = {\n",
    "    'gpu_ids': [0],\n",
    "    'isTrain': True,\n",
    "    'use_dropout': True,\n",
    "    'c_input': 3,\n",
    "    'c_output': 3,\n",
    "    'n_downsampling': 8,\n",
    "    'nd_filters': 64,\n",
    "    'ng_filters': 64,\n",
    "    'nd_layers': 3,\n",
    "    'lr':0.0002,\n",
    "    'beta1': 0.5,\n",
    "    'lambda_L1': 100\n",
    "}\n",
    "model = Pix2Pix(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e75Hl16MJQbP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def get_meta(root_dir):\n",
    "    \"\"\" Fetches the meta data for all the images and assigns labels.\n",
    "    \"\"\"\n",
    "    paths = []\n",
    "    \n",
    "    for entry in os.scandir(root_dir):\n",
    "        if (entry.is_file()):\n",
    "            paths.append(entry.path)\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-_ZYr98-JQbS"
   },
   "outputs": [],
   "source": [
    "root_dir_A = \"./data/trainA/\"\n",
    "root_dir_B = \"./data/trainB/\"\n",
    "\n",
    "paths_A = get_meta(root_dir_A)\n",
    "paths_B = get_meta(root_dir_B)\n",
    "\n",
    "data = {\n",
    "    'A': paths_A,\n",
    "    'B': paths_B\n",
    "}\n",
    "data_df = pd.DataFrame(data, columns=['A', 'B'])\n",
    "data_df = data_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "pXo5teKMJQbW",
    "outputId": "8ad18ff7-ea95-423b-e971-cb569effb1d4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./data/trainA/se2834_69.jpg</td>\n",
       "      <td>./data/trainB/se2834_69.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./data/trainA/se2934_142.jpg</td>\n",
       "      <td>./data/trainB/se2934_142.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./data/trainA/se3132_37.jpg</td>\n",
       "      <td>./data/trainB/se3132_37.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./data/trainA/se2838_127.jpg</td>\n",
       "      <td>./data/trainB/se2838_127.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./data/trainA/se2734_13.jpg</td>\n",
       "      <td>./data/trainB/se2734_13.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              A                             B\n",
       "0   ./data/trainA/se2834_69.jpg   ./data/trainB/se2834_69.jpg\n",
       "1  ./data/trainA/se2934_142.jpg  ./data/trainB/se2934_142.jpg\n",
       "2   ./data/trainA/se3132_37.jpg   ./data/trainB/se3132_37.jpg\n",
       "3  ./data/trainA/se2838_127.jpg  ./data/trainB/se2838_127.jpg\n",
       "4   ./data/trainA/se2734_13.jpg   ./data/trainB/se2734_13.jpg"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kSQqIkzxJQbc"
   },
   "outputs": [],
   "source": [
    "def compute_img_mean_std(image_paths):\n",
    "    \"\"\"\n",
    "        Author: @xinruizhuang. Computing the mean and std of three channel on the whole dataset,\n",
    "        first we should normalize the image from 0-255 to 0-1\n",
    "    \"\"\"\n",
    "\n",
    "    img_h, img_w = 256, 256\n",
    "    imgs = []\n",
    "    means, stdevs = [], []\n",
    "\n",
    "    for i in tqdm(range(len(image_paths))):\n",
    "        img = cv2.imread(image_paths[i])\n",
    "        img = cv2.resize(img, (img_h, img_w))\n",
    "        imgs.append(img)\n",
    "\n",
    "    imgs = np.stack(imgs, axis=3)\n",
    "    print(imgs.shape)\n",
    "\n",
    "    imgs = imgs.astype(np.float32) / 255.\n",
    "\n",
    "    for i in range(3):\n",
    "        pixels = imgs[:, :, i, :].ravel()  # resize to one row\n",
    "        means.append(np.mean(pixels))\n",
    "        stdevs.append(np.std(pixels))\n",
    "\n",
    "    means.reverse()  # BGR --> RGB\n",
    "    stdevs.reverse()\n",
    "\n",
    "    print(\"normMean = {}\".format(means))\n",
    "    print(\"normStd = {}\".format(stdevs))\n",
    "    return means, stdevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 117,
     "referenced_widgets": [
      "4fe5904169f146cbb143f3307df59558",
      "7a1c2ffc2224402a8fddf690daf4cce5",
      "bcf165af1e9d43bcbc52d24147edfa81",
      "a82d62f05f6e4ca398a7b3adc5aa96bf",
      "62c0f4a0be154169b292669d98229b0a",
      "9f130a65ed5548ab85205877266281bb",
      "3f26075455d94da7a1b603485041dbdb",
      "54f5add4e3814e5393329522367f8133"
     ]
    },
    "colab_type": "code",
    "id": "vUo_53PyJQbh",
    "outputId": "1b304ce0-c2fa-4c3a-dd94-9ba2377b6f19"
   },
   "outputs": [],
   "source": [
    "#norm_mean_A, norm_std_A = compute_img_mean_std(paths_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 117,
     "referenced_widgets": [
      "c1144c82dbf2403792c0f1ac58a8ddb2",
      "cd18e6581c044a87bbe8546a61f6b067",
      "6ee4fb7bf0464b2cb9166033f840be4b",
      "f356bbfb6c5b46e0b13a626e3e553340",
      "42e6d74442fe41c181fee938b18ef6fc",
      "37eca9109d204ce7bce9b18848ffa77f",
      "7a1ff598d11e4798a60cf611ce1130b1",
      "b83ace471d124c8d978d1f07bf57e873"
     ]
    },
    "colab_type": "code",
    "id": "M-vet3frJQbm",
    "outputId": "63de5b5c-cc86-4204-91c7-77dbcb77624a"
   },
   "outputs": [],
   "source": [
    "#norm_mean_B, norm_std_B = compute_img_mean_std(paths_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ybIWk3j8JQbs"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Streets(Dataset):\n",
    "    def __init__(self, df, transform_A, transform_B):\n",
    "        self.df = df\n",
    "        self.transform_A = transform_A\n",
    "        self.transform_B = transform_B\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, index):\n",
    "        x = Image.open(open(self.df['A'][index], 'rb')) \n",
    "        y = Image.open(open(self.df['B'][index], 'rb')) \n",
    "        \n",
    "        x = self.transform_A(x)\n",
    "        y = self.transform_B(y)\n",
    "        return x, y \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tSxEzlf3JQby"
   },
   "outputs": [],
   "source": [
    "data_transform_A = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "# Should we use a transform for B too? (Yes, they do)\n",
    "data_transform_B = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X272MIhdJQb2"
   },
   "outputs": [],
   "source": [
    "train_split = 0.70 # Defines the ratio of train/valid/test data.\n",
    "valid_split = 0.10\n",
    "\n",
    "train_size = int(len(data_df)*train_split)\n",
    "valid_size = int(len(data_df)*valid_split)\n",
    "\n",
    "ins_dataset_train = Streets(\n",
    "    df=data_df[:train_size],\n",
    "    transform_A=data_transform_A,\n",
    "    transform_B=data_transform_B\n",
    ")\n",
    "\n",
    "ins_dataset_valid = Streets(\n",
    "    df=data_df[train_size:(train_size + valid_size)].reset_index(drop=True),\n",
    "    transform_A=data_transform_A,\n",
    "    transform_B=data_transform_B\n",
    ")\n",
    "\n",
    "ins_dataset_test = Streets(\n",
    "    df=data_df[(train_size + valid_size):].reset_index(drop=True),\n",
    "    transform_A=data_transform_A,\n",
    "    transform_B=data_transform_B\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w-3_ErYHJQb7"
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_loader = DataLoader(dataset=ins_dataset_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=ins_dataset_test, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = DataLoader(dataset=ins_dataset_valid, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XYEmfOOLUsPL"
   },
   "outputs": [],
   "source": [
    "def tensor2im(input_image, imtype=np.uint8):\n",
    "    \"\"\"\"Converts a Tensor array into a numpy image array.\n",
    "    Parameters:\n",
    "        input_image (tensor) --  the input image tensor array\n",
    "        imtype (type)        --  the desired type of the converted numpy array\n",
    "    \"\"\"\n",
    "    if not isinstance(input_image, np.ndarray):\n",
    "        if isinstance(input_image, torch.Tensor):  # get the data from a variable\n",
    "            image_tensor = input_image.data\n",
    "        else:\n",
    "            return input_image\n",
    "        image_numpy = image_tensor[0].cpu().float().numpy()  # convert it into a numpy array\n",
    "        if image_numpy.shape[0] == 1:  # grayscale to RGB\n",
    "            image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
    "        image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0  # post-processing: tranpose and scaling\n",
    "    else:  # if it is a numpy array, do nothing\n",
    "        image_numpy = input_image\n",
    "    return image_numpy.astype(imtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pix2Pix(\n",
      "  (netG): DataParallel(\n",
      "    (module): UnetGenerator(\n",
      "      (model): UnetSkipConnectionBlock(\n",
      "        (model): Sequential(\n",
      "          (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (1): UnetSkipConnectionBlock(\n",
      "            (model): Sequential(\n",
      "              (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "              (1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (3): UnetSkipConnectionBlock(\n",
      "                (model): Sequential(\n",
      "                  (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                  (1): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                  (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (3): UnetSkipConnectionBlock(\n",
      "                    (model): Sequential(\n",
      "                      (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                      (1): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                      (3): UnetSkipConnectionBlock(\n",
      "                        (model): Sequential(\n",
      "                          (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                          (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                          (3): UnetSkipConnectionBlock(\n",
      "                            (model): Sequential(\n",
      "                              (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                              (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                              (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                              (3): UnetSkipConnectionBlock(\n",
      "                                (model): Sequential(\n",
      "                                  (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                                  (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                                  (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                                  (3): UnetSkipConnectionBlock(\n",
      "                                    (model): Sequential(\n",
      "                                      (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                                      (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                                      (2): ReLU(inplace=True)\n",
      "                                      (3): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                                      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                                    )\n",
      "                                  )\n",
      "                                  (4): ReLU(inplace=True)\n",
      "                                  (5): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                                  (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                                )\n",
      "                              )\n",
      "                              (4): ReLU(inplace=True)\n",
      "                              (5): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                              (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                            )\n",
      "                          )\n",
      "                          (4): ReLU(inplace=True)\n",
      "                          (5): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                          (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                        )\n",
      "                      )\n",
      "                      (4): ReLU(inplace=True)\n",
      "                      (5): ConvTranspose2d(1024, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                      (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (4): ReLU(inplace=True)\n",
      "                  (5): ConvTranspose2d(512, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                  (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                )\n",
      "              )\n",
      "              (4): ReLU(inplace=True)\n",
      "              (5): ConvTranspose2d(256, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "          (4): Tanh()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (netD): DataParallel(\n",
      "    (module): NLayerDiscriminator(\n",
      "      (model): Sequential(\n",
      "        (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "        (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "        (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "        (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "        (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lossGAN): GANLoss(\n",
      "    (loss): BCEWithLogitsLoss()\n",
      "  )\n",
      "  (lossL1): L1Loss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "97ec30ee19b844aea69c923984893d88",
      "1c27b634a3c540e19f4049da648169db",
      "f790f6cb69d14635929c668a433087c5",
      "1f937c541c5349e9bbd769f4bd45a5d6",
      "77ae3bbb64c7418f8c5e711348968ae9",
      "27996ba1b5f641ccb1f0197216e6f6e2",
      "c4f20b20c9d8439d8471412b9d5c690e",
      "0c22c2e718b1416eb00bf593b29e23d1"
     ]
    },
    "colab_type": "code",
    "id": "VrVl2YRvJQcC",
    "outputId": "e13bd664-d7d8-4ebd-9ab6-e66ca46d5f07"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da02dc062fb4420c8284714bbe520a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5714.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'Outputs' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_AB' image shape:  torch.Size([1, 6, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'fake_prediction' image shape:  torch.Size([1, 1, 30, 30]) \n",
      "###################\n",
      "###################\n",
      " 'A' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "###################\n",
      " 'B' image shape:  torch.Size([1, 3, 256, 256]) \n",
      "###################\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-b3e3cb368053>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'###################\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"'B' image shape: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n###################\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Run the forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    loss_D_list = []\n",
    "    loss_G_list = []\n",
    "    acc_list = []\n",
    "    for i, (inputs, targets) in enumerate(tqdm(train_loader)):\n",
    "        print('###################\\n', \"'A' image shape: \", str(inputs.shape), \"\\n###################\")\n",
    "        print('###################\\n', \"'B' image shape: \", str(targets.shape), \"\\n###################\")\n",
    "\n",
    "        inputs = inputs.to(model.device)\n",
    "        targets = targets.to(model.device)\n",
    "        # Run the forward pass\n",
    "        outputs = model(inputs)\n",
    "        print('###################\\n', \"'Outputs' image shape: \", str(outputs.shape), \"\\n###################\")\n",
    "\n",
    "        # update D\n",
    "        model.set_requires_grad(model.netD, True)  # enable backprop for D\n",
    "        model.optimizer_D.zero_grad()     # set D's gradients to zero\n",
    "        inputs_D_fake = torch.cat((inputs, outputs), 1)  # we use conditional GANs; we need to feed both input and output to the discriminator\n",
    "        print('###################\\n', \"'fake_AB' image shape: \", str(inputs_D_fake.shape), \"\\n###################\")\n",
    "\n",
    "        # Fake; stop backprop to the generator by detaching fake_B\n",
    "        \n",
    "        outputs_D_fake = model.netD(inputs_D_fake.detach())\n",
    "        print('###################\\n', \"'fake_prediction' image shape: \", str(outputs_D_fake.shape), \"\\n###################\")\n",
    "\n",
    "        loss_D_fake = model.lossGAN(outputs_D_fake, False)\n",
    "        # Real\n",
    "        inputs_D_real = torch.cat((inputs, targets), 1)\n",
    "        outputs_D_real = model.netD(inputs_D_real)\n",
    "        loss_D_real = model.lossGAN(outputs_D_real, True)\n",
    "        # combine loss and calculate gradients\n",
    "        loss_D = (loss_D_fake + loss_D_real) * 0.5\n",
    "        loss_D_list.append(loss_D.item())\n",
    "        loss_D.backward()\n",
    "        model.optimizer_D.step()          # update D's weights\n",
    "\n",
    "        \n",
    "        # update G\n",
    "        model.set_requires_grad(model.netD, False)  # D requires no gradients when optimizing G\n",
    "        model.optimizer_G.zero_grad()        # set G's gradients to zero\n",
    "        \n",
    "        inputs_D_fake = torch.cat((inputs, targets), 1)\n",
    "        outputs_D_fake = model.netD(inputs_D_fake)\n",
    "        loss_G_GAN = model.lossGAN(outputs_D_fake, True)\n",
    "        # Second, G(A) = B\n",
    "        loss_G_L1 = model.lossL1(outputs, targets) * model.opt['lambda_L1']        # combine loss and calculate gradients\n",
    "        loss_G = loss_G_GAN + loss_G_L1\n",
    "        loss_G_list.append(loss_G.item())\n",
    "        loss_G.backward()\n",
    "        model.optimizer_G.step()             # udpate G's weights\n",
    "        \n",
    "        \n",
    "        # Track the accuracy\n",
    "        \n",
    "    print('Epoch [{}/{}], Loss_D: {:.4f},  Loss_G: {:.4f}'\n",
    "              .format(epoch + 1, num_epochs, loss_D_list[i],  loss_G_list[i]))\n",
    "    if epoch % 10 == 0:\n",
    "        img_A = Image.open(paths_A[600])\n",
    "        img = data_transform_A(img_A)\n",
    "        img = torch.from_numpy(np.expand_dims(img, axis=0))\n",
    "        img = img.to(model.device)\n",
    "        out = model(img)\n",
    "        img_B_fake = tensor2im(out)\n",
    "        img_B_real = Image.open(paths_B[600])\n",
    "        img = np.concatenate((img_A, img_B_fake, img_B_real), axis=1) \n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        plt.imsave('results/{}.jpg'.format(epoch), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "uiTly5ziUrJF",
    "outputId": "96f8090c-0c44-4eb1-86c7-76454c40aae8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 256])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader.dataset[500][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-d2acd9137ef6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths_A\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_transform_A\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "img_A = Image.open(paths_A[9000])\n",
    "img = data_transform_A(img_A)\n",
    "img = torch.from_numpy(np.expand_dims(img, axis=0))\n",
    "img = img.to(model.device)\n",
    "out = model(img)\n",
    "print(out.shape)\n",
    "img_B_fake = tensor2im(out)\n",
    "img_B_real = Image.open(paths_B[600])\n",
    "img = np.concatenate((img_A, img_B_fake, img_B_real), axis=1)\n",
    "plt.imsave('results/test.png'.format(epoch), img)\n",
    "plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8164"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paths_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DeepStreets.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0c22c2e718b1416eb00bf593b29e23d1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c27b634a3c540e19f4049da648169db": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f937c541c5349e9bbd769f4bd45a5d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c22c2e718b1416eb00bf593b29e23d1",
      "placeholder": "",
      "style": "IPY_MODEL_c4f20b20c9d8439d8471412b9d5c690e",
      "value": " 259/1000 [05:11&lt;14:47,  1.20s/it]"
     }
    },
    "27996ba1b5f641ccb1f0197216e6f6e2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "37eca9109d204ce7bce9b18848ffa77f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f26075455d94da7a1b603485041dbdb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "42e6d74442fe41c181fee938b18ef6fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "4fe5904169f146cbb143f3307df59558": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bcf165af1e9d43bcbc52d24147edfa81",
       "IPY_MODEL_a82d62f05f6e4ca398a7b3adc5aa96bf"
      ],
      "layout": "IPY_MODEL_7a1c2ffc2224402a8fddf690daf4cce5"
     }
    },
    "54f5add4e3814e5393329522367f8133": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62c0f4a0be154169b292669d98229b0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "6ee4fb7bf0464b2cb9166033f840be4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_37eca9109d204ce7bce9b18848ffa77f",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_42e6d74442fe41c181fee938b18ef6fc",
      "value": 100
     }
    },
    "77ae3bbb64c7418f8c5e711348968ae9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "7a1c2ffc2224402a8fddf690daf4cce5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a1ff598d11e4798a60cf611ce1130b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "97ec30ee19b844aea69c923984893d88": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f790f6cb69d14635929c668a433087c5",
       "IPY_MODEL_1f937c541c5349e9bbd769f4bd45a5d6"
      ],
      "layout": "IPY_MODEL_1c27b634a3c540e19f4049da648169db"
     }
    },
    "9f130a65ed5548ab85205877266281bb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a82d62f05f6e4ca398a7b3adc5aa96bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54f5add4e3814e5393329522367f8133",
      "placeholder": "",
      "style": "IPY_MODEL_3f26075455d94da7a1b603485041dbdb",
      "value": " 100/100 [00:26&lt;00:00,  3.84it/s]"
     }
    },
    "b83ace471d124c8d978d1f07bf57e873": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bcf165af1e9d43bcbc52d24147edfa81": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9f130a65ed5548ab85205877266281bb",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_62c0f4a0be154169b292669d98229b0a",
      "value": 100
     }
    },
    "c1144c82dbf2403792c0f1ac58a8ddb2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6ee4fb7bf0464b2cb9166033f840be4b",
       "IPY_MODEL_f356bbfb6c5b46e0b13a626e3e553340"
      ],
      "layout": "IPY_MODEL_cd18e6581c044a87bbe8546a61f6b067"
     }
    },
    "c4f20b20c9d8439d8471412b9d5c690e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cd18e6581c044a87bbe8546a61f6b067": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f356bbfb6c5b46e0b13a626e3e553340": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b83ace471d124c8d978d1f07bf57e873",
      "placeholder": "",
      "style": "IPY_MODEL_7a1ff598d11e4798a60cf611ce1130b1",
      "value": " 100/100 [00:01&lt;00:00, 60.18it/s]"
     }
    },
    "f790f6cb69d14635929c668a433087c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": " 26%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_27996ba1b5f641ccb1f0197216e6f6e2",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_77ae3bbb64c7418f8c5e711348968ae9",
      "value": 259
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
